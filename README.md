

## Assignment 3

### Image Captioning
#### Coco Dataset for Captioning

Images features, dim=4096, are extracted from the fc7 layer of the VGG-16 network pretrained on ImageNet (not COCO).
And further, a dimensionality reduced version with dim=512 is generated by PCA.

Word to integer index and integer index to word mappings are stored in a JSON file.
4 special tokes are added, `<START>`, `<END>`, `<UNK>` for out-of-vocab words, and `<NULL>` for padding.


#### RNN and LSTM Captioning

Vanilla RNN and LSTM are implemented using purely Numpy.

The experiments overfit 50 training data points from COCO. Somehow in this setup, the LSTM's performance is not as good as RNN.


#### Transformer Captioning

The transformer for image captioning is decoder only. Because the image features are used as output of encoder, and are attended at every decoder layer (block).
Each decoder block has 4 layers, a self-attention layer, a multi-head attention layer (to attend image features), followed by two linear layers. 

At training time, for each caption x, `x[:-1]` is used as input, and `x[1:]` is the target. `x[:-1]` goes through an embedding layer and positional embedding layer before feeding into the first decoder layer.
The captions in a batch are packed with the token `<NULL>` so they have the same length. The `<NULL>` tokens do not contribute to loss and as a result no gradients.

At test time (sampling), the first input is the `<START>` token, and tokens generated at each step are concatenated together and fed back as input until we have reached the `max_len` number of tokens or `<END>` is produced. 

### GAN



### Self-supervised Learning


